{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 6:  Convexity and Optimization\n",
    "\n",
    "*Matteo Courthoud*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "- [Gradient Descent](#/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-780e8ed61909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autograd'"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import matplotlib for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# Set global parameters\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['figure.titlesize'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Function to plot errors\n",
    "def error_plot(ys, yscale='log'):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Error')\n",
    "    plt.yscale(yscale)\n",
    "    plt.plot(range(len(ys)), ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.1 Gradient Descent\n",
    "\n",
    "[*go to index*](#/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We start with a basic implementation of projected gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(init, steps, grad, proj=lambda x: x):\n",
    "    \"\"\"Projected gradient descent.\n",
    "    \n",
    "    Inputs:\n",
    "        initial: starting point\n",
    "        steps: list of scalar step sizes\n",
    "        grad: function mapping points to gradients\n",
    "        proj (optional): function mapping points to points\n",
    "        \n",
    "    Returns:\n",
    "        List of all points computed by projected gradient descent.\n",
    "    \"\"\"\n",
    "    xs = [init]\n",
    "    for step in steps:\n",
    "        xs.append(proj(xs[-1] - step * grad(xs[-1])))\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this implementation keeps around all points computed along the way. This is clearly not what you would do on large instances. We do this for illustrative purposes to be able to easily inspect the computed sequence of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"warmup\"></a>\n",
    "## Warm-up: Optimizing a quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As a toy example, let's optimize $$f(x)=\\frac12\\|x\\|^2,$$ which has the gradient map $\\nabla f(x)=x.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(x):\n",
    "    return 0.5*x.dot(x)\n",
    "\n",
    "def quadratic_gradient(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note the function is $1$-smooth and $1$-strongly convex. Our theorems would then suggest that we use a constant step size of $1.$ If you think about it, for this step size the algorithm will actually find the optimal solution in just one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.random.normal(0, 1, (1000))\n",
    "_, x1 = gradient_descent(x0, [1.0], quadratic_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Indeed, it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.all() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we don't have the right learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = gradient_descent(x0, [0.1]*50, quadratic_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot errors along steps\n",
    "error_plot([quadratic(x) for x in xs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's say we want to optimize the function inside some affine subspace. Recall that affine subspaces are convex sets. Below we pick a random low dimensional affine subspace $b+U$ and define the corresponding linear projection operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U is an orthonormal basis of a random 100-dimensional subspace.\n",
    "U = np.linalg.qr(np.random.normal(0, 1, (1000, 100)))[0]\n",
    "b = np.random.normal(0, 1, 1000)\n",
    "\n",
    "def proj(x):\n",
    "    \"\"\"Projection of x onto an affine subspace\"\"\"\n",
    "    return b + U.dot(U.T).dot(x-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.random.normal(0, 1, (1000))\n",
    "xs = gradient_descent(x0, [0.1]*50, quadratic_gradient, proj)\n",
    "# the optimal solution is the projection of the origin\n",
    "x_opt = proj(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "error_plot([quadratic(x) for x in xs])\n",
    "plt.plot(range(len(xs)), [quadratic(x_opt)]*len(xs),\n",
    "        label='$\\\\frac{1}{2}|\\!|x_{\\mathrm{opt}}|\\!|^2$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orangle line shows the optimal error, which the algorithm reaches quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The iterates also converge to the optimal solution in domain as the following plot shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot([np.linalg.norm(x_opt-x)**2 for x in xs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Least Squares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One of the most fundamental data analysis tools is *linear least squares*. Given an $m\\times n$ matrix $A$ and a vector $b$ our goal is to find a vector $x\\in\\mathbb{R}^n$ that minimizes the following objective: \n",
    "\n",
    "<p>\n",
    "$$f(x) = \\frac 1{2m}\\sum_{i=1}^m (a_i^\\top x - b_j)^2 \n",
    "=\\frac1{2m}\\|Ax-b\\|^2$$\n",
    "</p>\n",
    "\n",
    "We can verify that $\\nabla f(x) = A^\\top(Ax-b)$ and\n",
    "$\\nabla^2 f(x) = A^\\top A.$\n",
    "\n",
    "Hence, the objective is $\\beta$-smooth with \n",
    "$\\beta=\\lambda_{\\mathrm{max}}(A^\\top A)$, and $\\alpha$-strongly convex with $\\alpha=\\lambda_{\\mathrm{min}}(A^\\top A)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(A, b, x):\n",
    "    \"\"\"Least squares objective.\"\"\"\n",
    "    return (0.5/m) * np.linalg.norm(A.dot(x)-b)**2\n",
    "\n",
    "def least_squares_gradient(A, b, x):\n",
    "    \"\"\"Gradient of least squares objective at x.\"\"\"\n",
    "    return A.T.dot(A.dot(x)-b)/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overdetermined case $m\\ge n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 1000, 100\n",
    "A = np.random.normal(0, 1, (m, n))\n",
    "x_opt = np.random.normal(0, 1, n)\n",
    "noise = np.random.normal(0, 0.1, m)\n",
    "b = A.dot(x_opt) + noise\n",
    "objective = lambda x: least_squares(A, b, x)\n",
    "gradient = lambda x: least_squares_gradient(A, b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convergence in objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.random.normal(0, 1, n)\n",
    "xs = gradient_descent(x0, [0.1]*100, gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot([objective(x) for x in xs])\n",
    "plt.plot(range(len(xs)), [np.linalg.norm(noise)**2]*len(xs),\n",
    "        label='noise level')\n",
    "plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),\n",
    "        label='optimal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convergence in domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underdetermined case $m < n$\n",
    "\n",
    "In the underdetermined case, the least squares objective is inevitably not strongly convex, since $A^\\top A$ is a rank deficient matrix and hence $\\lambda_{\\mathrm{min}}(A^\\top A)=0.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 100, 1000\n",
    "A = np.random.normal(0, 1, (m, n))\n",
    "b = np.random.normal(0, 1, m)\n",
    "# The least norm solution is given by the pseudo-inverse\n",
    "x_opt = np.linalg.pinv(A).dot(b)\n",
    "objective = lambda x: least_squares(A, b, x)\n",
    "gradient = lambda x: least_squares_gradient(A, b, x)\n",
    "x0 = np.random.normal(0, 1, n)\n",
    "xs = gradient_descent(x0, [0.1]*100, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot([objective(x) for x in xs])\n",
    "plt.plot(range(len(xs)), [least_squares(A,b,x_opt)]*len(xs),\n",
    "        label='optimal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While we quickly reduce the error, we don't actually converge in domain to the least norm solution. This is just because the function is no longer strongly convex in the underdetermined case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot([np.linalg.norm(x-x_opt)**2 for x in xs], yscale='linear')\n",
    "plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),\n",
    "         label='$|\\!|x_{\\mathrm{opt}}|\\!|^2$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"regularization\"></a>\n",
    "\n",
    "## $\\ell_2$-regularized least squares\n",
    "\n",
    "In the underdetermined case, it is often desirable to restore strong convexity of the objective function by adding an $\\ell_2^2$-penality, also known as *Tikhonov regularization*, $\\ell_2$-regularization, or *weight decay*.\n",
    "\n",
    "<p>\n",
    "$$\\frac1{2m}\\|Ax-b\\|^2 + \\frac{\\alpha}2\\|x\\|^2$$\n",
    "</p>\n",
    "\n",
    "Note: With this modification the objective is $\\alpha$-strongly convex again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_l2(A, b, x, alpha=0.1):\n",
    "    return least_squares(A, b, x) + (alpha/2) * x.dot(x)\n",
    "\n",
    "def least_squares_l2_gradient(A, b, x, alpha=0.1):\n",
    "    return least_squares_gradient(A, b, x) + alpha * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create a least squares instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 100, 1000\n",
    "A = np.random.normal(0, 1, (m, n))\n",
    "b = A.dot(np.random.normal(0, 1, n))\n",
    "objective = lambda x: least_squares_l2(A, b, x)\n",
    "gradient = lambda x: least_squares_l2_gradient(A, b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that we can find the optimal solution to the optimization problem in closed form without even running gradient descent by computing $x_{\\mathrm{opt}}=(A^\\top+\\alpha I)^{-1}A^\\top b.$ Please verify that this point is indeed optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt = np.linalg.inv(A.T.dot(A) + 0.1*np.eye(1000)).dot(A.T).dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how gradient descent fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.random.normal(0, 1, n)\n",
    "xs = gradient_descent(x0, [0.1]*500, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We plot the descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot([objective(x) for x in xs])\n",
    "plt.plot(range(len(xs)), [least_squares_l2(A,b,x_opt)]*len(xs),\n",
    "        label='optimal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the error doesn't decrease below a certain level due to the regularization term. This is not a bad thing. In fact, the regularization term gives as *strong convexity* which leads to convergence in domain again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Boh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = gradient_descent(x0, [0.1]*500, gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot([np.linalg.norm(x-x_opt)**2 for x in xs])\n",
    "plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),\n",
    "        label='squared norm of $x_{\\mathrm{opt}}$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"implicit\"></a>\n",
    "## The magic of implicit regularization\n",
    "\n",
    "Sometimes simply running gradient descent from a suitable initial point has a regularizing effect on its own **without introducing an explicit regularization term**.\n",
    "\n",
    "We will see this below where we revisit the unregularized least squares objective, but initialize gradient descent from the origin rather than a random gaussian point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize from 0\n",
    "x0 = np.zeros(n)\n",
    "# Note this is the gradient w.r.t. the unregularized objective!\n",
    "gradient = lambda x: least_squares_gradient(A, b, x)\n",
    "xs = gradient_descent(x0, [0.1]*50, gradient)\n",
    "error_plot([np.linalg.norm(x_opt-x)**2 for x in xs], yscale='linear')\n",
    "plt.plot(range(len(xs)), [np.linalg.norm(x_opt)**2]*len(xs),\n",
    "         label='$|\\!|x_{\\mathrm{opt}}|\\!|^2$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Incredible!* We converge to the minimum norm solution!\n",
    "\n",
    "Implicit regularization is a deep phenomenon that's an active research topic in learning and optimization. It's exciting that we see it play out in this simple least squares problem already!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"lasso\"></a>\n",
    "## LASSO\n",
    "\n",
    "LASSO is the name for $\\ell_1$-regularized least squares regression:\n",
    "\n",
    "<p>\n",
    "$$\\frac1{2m}\\|Ax-b\\|^2 + \\alpha\\|x\\|_1$$\n",
    "</p>\n",
    "\n",
    "We will see that LASSO is able to fine *sparse* solutions if they exist. This is a common motivation for using an $\\ell_1$-regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(A, b, x, alpha=0.1):\n",
    "    return least_squares(A, b, x) + alpha * np.linalg.norm(x, 1)\n",
    "\n",
    "def ell1_subgradient(x):\n",
    "    \"\"\"Subgradient of the ell1-norm at x.\"\"\"\n",
    "    g = np.ones(x.shape)\n",
    "    g[x < 0.] = -1.0\n",
    "    return g\n",
    "\n",
    "def lasso_subgradient(A, b, x, alpha=0.1):\n",
    "    \"\"\"Subgradient of the lasso objective at x\"\"\"\n",
    "    return least_squares_gradient(A, b, x) + alpha*ell1_subgradient(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m, n = 100, 1000\n",
    "A = np.random.normal(0, 1, (m, n))\n",
    "x_opt = np.zeros(n)\n",
    "x_opt[:10] = 1.0\n",
    "b = A.dot(x_opt)\n",
    "x0 = np.random.normal(0, 1, n)\n",
    "xs = gradient_descent(x0, [0.1]*500, lambda x: lasso_subgradient(A, b, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot([lasso(A, b, x) for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Comparison of initial, optimal, and computed point')\n",
    "idxs = range(50)\n",
    "plt.plot(idxs, x0[idxs], '--', color='#aaaaaa', label='initial')\n",
    "plt.plot(idxs, x_opt[idxs], 'r-', label='optimal')\n",
    "plt.plot(idxs, xs[-1][idxs], 'g-', label='final')\n",
    "plt.xlabel('Coordinate')\n",
    "plt.ylabel('Value')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, LASSO correctly identifies the significant coordinates of the optimal solution. This is why, in practice, LASSO is a popular tool for feature selection.\n",
    "\n",
    "Play around with this plot to inspect other points along the way, e.g., the point that achieves lowest objective value. Why does the objective value go up even though we continue to get better solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"svm\"></a>\n",
    "## Support Vector Machines\n",
    "\n",
    "In a linear classification problem, we're given $m$ labeled points $(a_i, y_i)$ and we wish to find a hyperplane given by a point $x$ that separates them so that \n",
    "\n",
    "* $\\langle a_i, x\\rangle \\ge 1$ when $y_i=1$, and \n",
    "* $\\langle a_i, x\\rangle \\le -1$ when $y_i = -1$\n",
    "\n",
    "The smaller the norm $\\|x\\|$ the larger the *margin* between positive and negative instances. Therefore, it makes sense to throw in a regularizer that penalizes large norms. This leads to the objective.\n",
    "\n",
    "<p>\n",
    "$$\\frac 1m \\sum_{i=1}^m \\max\\{1-y_i(a_i^\\top x), 0\\} + \\frac{\\alpha}2\\|x\\|^2$$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(z):\n",
    "    return np.maximum(1.-z, np.zeros(z.shape))\n",
    "\n",
    "def svm_objective(A, y, x, alpha=0.1):\n",
    "    \"\"\"SVM objective.\"\"\"\n",
    "    m, _ = A.shape\n",
    "    return np.mean(hinge_loss(np.diag(y).dot(A.dot(x))))+(alpha/2)*x.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-2, 2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(z, hinge_loss(z));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def hinge_subgradient(z):\n",
    "    g = np.zeros(z.shape)\n",
    "    g[z < 1] = -1.\n",
    "    return g\n",
    "\n",
    "def svm_subgradient(A, y, x, alpha=0.1):\n",
    "    g1 = hinge_subgradient(np.diag(y).dot(A.dot(x)))\n",
    "    g2 = np.diag(y).dot(A)\n",
    "    return g1.dot(g2) + alpha*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(z, hinge_subgradient(z));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m, n = 1000, 100\n",
    "A = np.vstack([np.random.normal(0.1, 1, (m//2, n)),\n",
    "               np.random.normal(-0.1, 1, (m//2, n))])\n",
    "y = np.hstack([np.ones(m//2), -1.*np.ones(m//2)])\n",
    "x0 = np.random.normal(0, 1, n)\n",
    "xs = gradient_descent(x0, [0.01]*100, \n",
    "                      lambda x: svm_subgradient(A, y, x, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot([svm_objective(A, y, x) for x in xs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see if averaging out the solutions gives us an improved function value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xavg = 0.0\n",
    "for x in xs:\n",
    "    xavg += x\n",
    "svm_objective(A, y, xs[-1]), svm_objective(A, y, xavg/len(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also look at the accuracy of our linear model for predicting the labels. From how we defined the data, we can see that the all ones vector is the highest accuracy classifier in the limit of infinite data (very large $m$). For a finite data set, the accuracy could be even higher due to random fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(A, y, x):\n",
    "    return np.mean(np.diag(y).dot(A.dot(x))>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Step')\n",
    "plt.plot(range(len(xs)), [accuracy(A, y, x) for x in xs])\n",
    "plt.plot(range(len(xs)), [accuracy(A, y, np.ones(n))]*len(xs),\n",
    "        label='Population optimum')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy spikes pretty early and drops a bit as we train for too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"inverse\"></a>\n",
    "## Sparse Inverse Covariance Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a positive semidefinite matrix $S\\in\\mathbb{R}^{n\\times n}$ the objective function in sparse inverse covariance estimation is as follows:\n",
    "\n",
    "<p>\n",
    "$$ \\min_{X\\in\\mathbb{R}^{n\\times n}, X\\succeq 0} \n",
    "\\langle S, X\\rangle - \\log\\det(X) + \\alpha\\|X\\|_1$$\n",
    "</p>\n",
    "\n",
    "Here, we define\n",
    "$$\\langle S, X\\rangle = \\mathrm{trace}(S^\\top X)$$\n",
    "and\n",
    "$$\\|X\\|_1 = \\sum_{ij}|X_{ij}|.$$\n",
    "\n",
    "Typically, we think of the matrix $S$ as a sample covariance matrix of a set of vectors $a_1,\\dots, a_m,$ defined as:\n",
    "$$\n",
    "S = \\frac1{m-1}\\sum_{i=1}^n a_ia_i^\\top\n",
    "$$\n",
    "The example also highlights the utility of automatic differentiation as provided by the `autograd` package that we'll regularly use. In a later lecture we will understand exactly how automatic differentiation works. For now we just treat it as a blackbox that gives us gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_inv_cov(S, X, alpha=0.1):\n",
    "    return (np.trace(S.T.dot(X))\n",
    "            - np.log(np.linalg.det(X))\n",
    "            + alpha * np.sum(np.abs(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "A = np.random.normal(0, 1, (n, n))\n",
    "S = A.dot(A.T)\n",
    "objective = lambda X: sparse_inv_cov(S, X)\n",
    "# autograd provides a \"gradient\", yay!\n",
    "gradient = grad(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also need to worry about the projection onto the positive semidefinite cone, which corresponds to truncating eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(X):\n",
    "    \"\"\"Projection onto positive semidefinite cone.\"\"\"\n",
    "    es, U = np.linalg.eig(X)\n",
    "    es[es<0] = 0.\n",
    "    return U.dot(np.diag(es).dot(U.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 = np.random.normal(0, 1, (n,n))\n",
    "X0 = A0.dot(A0.T)\n",
    "Xs = gradient_descent(X0, [0.01]*500, gradient, projection)\n",
    "error_plot([objective(X) for X in Xs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"madness\"></a>\n",
    "\n",
    "# Going crazy with autograd\n",
    "\n",
    "Just for fun, we'll go through a crazy example below. We can use `autograd` not just for getting gradients for natural objectives, we can in principle also use it to tune hyperparameters of our optimizer, like the step size schedulde.\n",
    "\n",
    "Below we see how we can find a better 10-step learning rate schedules for optimizing a quadratic. This is mostly just for illustrative purposes (although some researchers are exploring these kinds of ideas more seriously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2e8627a76c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x0 = np.random.normal(0, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.5*np.dot(x,x)\n",
    "\n",
    "def optimizer(steps):\n",
    "    \"\"\"Optimize a quadratic with the given steps.\"\"\"\n",
    "    xs = gradient_descent(x0, steps, grad(f))\n",
    "    return f(xs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function `optimizer` is a non-differentiable function of its input `steps`. Nontheless, `autograd` will provide a gradient that we can stick into gradient descent. That is, we're tuning gradient descent with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_optimizer = grad(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_steps = np.abs(np.random.normal(0, 0.1, 10))\n",
    "better_steps = gradient_descent(initial_steps, [0.001]*500, grad_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "error_plot([optimizer(steps) for steps in better_steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the learning rate schedules improve dramatically over time. Of course, we already know from the first example that there is a step size schedule that converges in one step. Interestingly, the last schedule we find here doesn't look at all like what we might expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xticks(range(len(better_steps[-1])))\n",
    "plt.ylabel('Step size')\n",
    "plt.xlabel('Step number')\n",
    "plt.plot(range(len(better_steps[-1])), better_steps[-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next Session\n",
    "\n",
    "Jump to [Session 7 - Trees and Forests](https://nbviewer.jupyter.org/github/matteocourthoud/Machine-Learning-for-Economic-Analysis-2020/blob/master/7_trees.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
