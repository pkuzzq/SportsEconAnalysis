---
title: "Problem Set 4, Solutions"
subtitle: "Nonstationarity, Causality, Instrumental Variables"
author: "**EC 421:** Introduction to Econometrics"
date: "<br>Due *before* midnight (11:59pm) on Wednesday, 05 June 2019"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      ratio: '8.5:11'
      # ratio: '8.8:11.4'
      # ratio: '8.4:10.87'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear

```{R, include = F}
library(pacman)
p_load(tidyverse, broom, kable, kableExtra, magrittr)
```

.mono[DUE] Your solutions to this problem set are due *before* midnight on Wednesday, 05 June 2019. Your files must be uploaded to [Canvas](https://canvas.uoregon.edu/).

.mono.b[IMPORTANT] Your submission must include (1) **your responses/answers to the question in a PDF, Word, or similar file** and (2) the .mono[R] script you used to generate your answers. **The .mono[R] script is just for your code. To receive credit, your answers/figures/*etc.* must be in the PDF/Word document.** Each student must turn in her/his own answers.

.mono[OBJECTIVE] This problem set has three purposes: (1) reinforce econometrics topics from class; (2) build your .mono[R] toolset; (3) strengthen your intuition on causality and time series.

## Problem 1: Nonstationarity—the Basics

**1a.** Define stationarity.

*Note:* You can define it using math or words (or both).

.pink[

**Answer:** *Stationarity* provides a concept of "well-behaved" time-series processes. We want our data to be *weakly persistant*, meaning periods that are far apart in time do not have a strong relationship. Stationarity formalizes this requirement. Specifically, stationarity means

1. The **mean** of our variable is independent of time, (*i.e.*, $\mathop{\boldsymbol{E}}\left[ x_t \right] = \mathop{\boldsymbol{E}}\left[ x_{t-k} \right]$ for any $k$)
2. The **variance** of the variable is independent of time (*i.e.*, $\mathop{\text{Var}} \left( x_t \right) = \mathop{\text{Var}} \left( x_{t-k} \right)$ for all $k$)
3. The **covariance** between two periods is independent of time (*i.e.*, $\mathop{\text{Cov}} \left( x_t,\,x_{t-k} \right) = \mathop{\text{Cov}} \left( x_s,\, x_{s-k} \right)$ for any $s$, $t$, and $k$)

]

**1b.** If our disturbance term $u_t$ follows a .pink[random walk], *i.e.*,
$$
\begin{align}
  u_{t} = u_{t-1} + \varepsilon_t
\end{align}
$$
then it's variance is $\mathop{\text{Var}} \left( u_t \right) = t \sigma_{\varepsilon}^2$. Explain how this expression of its variance shows that the disturbance is .purple[nonstationary] (*i.e.*, it violates .pink[stationarity]).

.pink[

**Answer:** The variance of a random walk clearly depends upon time—meaning it is **not** independent of time. In other words: As time increases, the variance increases.

]

**1c.** We previously discussed autocorrelated distrubances, *e.g.*, an AR(1) process such that
$$
\begin{align}
  u_{t} = \rho u_{t-1} + \varepsilon_t
\end{align}
$$
Under which circumstances would this AR(1) process become a random walk?

*Hint:* Consider the values of $\rho$.

.pink[

**Answer:** If $\rho=1$, then this AR(1) process becomes a random walk.

]

---
class: clear

## Problem 2: Nonstationarity—the Simulation

In this problem, we are going to create two independent, .hi-purple[nonstationary] time series. Specifically, we'll create two random walks. Then, we'll regress the first random walk on the second random walk.

*Hint:* Generating random walks is *nearly* identical to generating AR(1) processes, as you did in lab.

**2a.** Generate the first 50-period random walk. We will name it `v`.
$$
\begin{align}
  v_t = v_{t-1} + \varepsilon_t
\end{align}
$$
where $\varepsilon_t$ comes from a normal distribution with mean 0 and standard deviation 1.

Here is some .mono[R] to help.

```{R, 2a_help, eval = F}
# Set a seed (so your results stay the same)
set.seed(1234)
# Generate the initial number, (this will be v[1])
v <- rnorm(1, mean = 0, sd = 1)
# For loop to create the random walk
for (t in 2:50) {
  # Create the 'next' observation
  ...
}
```
while you're filling in the `for` loop, keep in mind (**1**) our equation for the random walk at the beginning of this question (meaning $v_t$ depends upon $v_{t-1}$ and $\varepsilon_t$) and (**2**) the fact that you can reference different observations in .mono[R], *e.g.*,

- `v[t]` refers to the $t$.super[th] observation
- `v[t-1]` refers to the $(t-1)$.super[th] observation
- `v[3]` refers to the $3$.super[rd] observation

If you need more help on for loops, don't forget there are lab materials on Canvas and resources online (*e.g.*, [datamentor.io](https://www.datamentor.io/r-programming/for-loop/) and [datacamp.com](https://www.datacamp.com/community/tutorials/tutorial-on-loops-in-r) have lots of resources).

.pink[

**Answer:** Here is .mono[R] code for our first random walk...

```{R, ans_2a}
# Set the seed
set.seed(1234)
# Generate the initial number, (this will be v[1])
v <- rnorm(1, mean = 0, sd = 1)
# For loop to create the random walk
for (t in 2:50) {
  # Create the 'next' observation
  v[t] <- v[t-1] + rnorm(1, mean = 0, sd = 1)
}
```

]

---
class: clear

**2b.** Generate a second 50-period random walk called `w`. This part is exactly the same as (2a), but you **use a different seed** (*i.e.*, `set.seed(456)`) and **name the variable** `w`.

.pink[

**Answer:** Here is .mono[R] code for our second random walk...

```{R, ans_2b}
# Set the seed
set.seed(5678)
# Generate the initial number, (this will be v[1])
w <- rnorm(1, mean = 0, sd = 1)
# For loop to create the random walk
for (t in 2:50) {
  # Create the 'next' observation
  w[t] <- w[t-1] + rnorm(1, mean = 0, sd = 1)
}
```

]

**2c.** We .orange[independently] generated these two time series. Ideally (from a statistical point of view), should we find a statistically significant relationship between the two series? Explain.

.pink[

**Answer:** If two variables are generated independently, then we ideally would not find a statistically significant relationship between them.

]

**2d.** Regress `w` on `v`. Report the results from the $t$ test. Do they match your expectations from (2c)?

.pink[

**Answer:** Regressing `w` on `v`

```{R, ans_2d}
# Regress w on v
reg_2d <- lm(w ~ v)
# 'tidy' results
reg_2d %>% tidy()
```

We estimate a coefficient of `r reg_2d$coefficients[2] %>% round(2)`, which has a *p*-value of approximately `r tidy(reg_2d)$p.value[2] %>% round(4)`. Thus, we find statistically significant evidence of a relationship (at the 5-percent level) despite the fact that there is no true relationship.

As we discussed in class, random walks—and other nonstationary processes—can lead to a higher probability of finding a spurious relationship.

**Note** Depending on the random numbers that you draw, you might not find evidence of a statistically significant relationship.

]

---
class: clear

## Problem 3: Causality

Following the Rubin causal model, imagine that we observe the following data (which would be impossible observe in real life):

.center[
.bold[Table: Imaginary dataset]
]
```{R, 3a_table, echo = F, warning = F, message = F, error = F}
rubin_df <- data.frame(
  i = 1:4,
  trt = rep(c(0,1), each = 2),
  y1 = 1 + 2 * c(12, 7, 5, 6),
  y0 = 1 + 2 * c(8, 5, 1, 4)
)
rubin_df %>%
kable(
  col.names = c(".math[i]", "Trt.", "y.sub[1]", "y.sub[0]"),
  escape = F
) %>%
kable_styling(full_width = F) %>%
row_spec(1:4, background = "white")
```

**3a.** Calculate the treatment effect **for each individaul** (*i.e.*, $\tau_i$).

.pink[**Answer:** The treatment effects for the individuals are 8, 4, 8, and 4.]

**3b.** **[T/F]** The treatment effect is constant across individuals.

.pink[**Answer:** False: the treatment effect varies across individuals.]

**3c.** Calculate the **average treatment effect**.

.pink[**Answer:** The average treatment effect is (8 + 4 + 8 + 4)/4 = 6.]

**3d** **Estimate the average treatment effect** by comparing the **mean of the treatment group** to the **mean of the control group**.

.pink[**Answer:** Our estimate of the average treatment effect is (17 + 11)/2 - (11 + 13)/2 = 2.]

**3e.** Should we expect our estimator in (3d) to provide unbiased estimates? **Explain.**

.pink[**Answer:** No! Unless we have a reason to believe that treatment was randomly distributed (or as-good-as randomly distributed), there is likely selection bias. Here, we can see that selection bias is very present: the y.sub[0] values for the treatment group are very different from the y.sub[0] values for the control group.]

**3f.** Why would it be impossible to actually observe all of the data in the table (in real life)?

.pink[**Answer:** We cannot observe the same individual (*i*) simultaneously receiving treatment and control. Thus, we will either observe y.[0] or y.[1]—not both.]

**3g.** How does your answer in (3f) relate to *the fundametal problem of causal inference*?

.pink[**Answer:** (3f) pretty much depicts the fundamental problem of causal inference: We cannot observe the same person with treatment and without treatment.]

---
class: clear

## Problem 4: Instrumental Variables

**4a.** What are the two requirements for a valid instrument?

.pink[

**Answer:** A valid instrument must be:

1. **Relevant**, *i.e.*, the instrument affects our endogenous variable $x$
1. **Exogenous**, *i.e.*, the instrument only affects our outcome variable $y$ through the endogenous variable $x$ ***and*** the instrument is uncorrelated with the disturbance $u$

]

**4b.** We're interested in estimating $\beta_1$ in
$$
\begin{align}
  \text{Wage}_i = \beta_0 + \beta_1 \text{Education}_i + u_i
\end{align}
$$
but we have a problem with omitted-variable bias. Instrumental variables can potentially help.

As we've discussed, we need an instrument for (endogenous) education. Do you think the number of children would be a valid instrument? Explain why it passes/fails each of the two requirements for a valid instrument.

.pink[

**Answer:** *Number of kids* is probably not a valid instrument.

While a person's number of children seems reasonably **relevant** for the person's educational level, it seems likely to be correlated with other omitted variables that are in the disturbance—*e.g.*, age, experience, parents' income. In addition, it even seems plausible that the number of children could directly affect a person's weekly wage and the available jobs, as number of children may affect the of hours a person is available to work. Thus, number of children is probably not **exogenous**.

]

**4c.** Which estimates would you trust more—OLS or IV, where number-of-children is your instrument? Explain.

.pink[

**Answer:** It's hard to know which would we should trust more. We probably don't want to put too much faith in either estimate: the OLS-based estimate is almost definitely suffers from omitted-variable bias, and the IV-based estimate is likely inconsistent due to the fact the *number of kids* is probably not a valid instrument.

]


```{R, generate pdfs, include = F, eval = T}
system("decktape remark ps04Solutions.html ps04Solutions.pdf --chrome-arg=--allow-file-access-from-files")
```
